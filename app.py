import streamlit as st
import os

# Tenta importar as bibliotecas LLM, lida com erros se n√£o estiverem instaladas
try:
    from openai import OpenAI, AzureOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    from litellm import completion
    LITELLM_AVAILABLE = True # Usado para Meta Llama e potencialmente outros
except ImportError:
    LITELLM_AVAILABLE = False

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="Gerador de Hist√≥rias Infantis", page_icon="üß∏", layout="centered")

# --- Modelos LLM ---
llm_providers = {
    "OpenAI": {
        "models": ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"],
        "available": OPENAI_AVAILABLE,
        "requires_endpoint": False
    },
    "Anthropic": {
        "models": ["claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-2.1"],
        "available": ANTHROPIC_AVAILABLE,
        "requires_endpoint": False
    },
    "Azure OpenAI": {
        "models": ["gpt-4", "gpt-35-turbo"], # Estes s√£o nomes de implanta√ß√£o, usu√°rio deve configurar
        "available": OPENAI_AVAILABLE, # Usa a mesma biblioteca base
        "requires_endpoint": True,
        "extra_fields": ["azure_endpoint", "azure_api_version", "azure_deployment_name"]
    },
    "Meta (via LiteLLM)": {
        "models": ["meta-llama/Llama-3-8b-chat-hf", "meta-llama/Llama-3-70b-chat-hf"], # Exemplos, requer configura√ß√£o no ambiente LiteLLM
        "available": LITELLM_AVAILABLE,
        "requires_endpoint": False # LiteLLM abstrai isso, mas pode precisar de vari√°veis de ambiente
    }
}

# --- Fun√ß√µes Auxiliares ---
def gerar_historia_com_llm(api_key, provider, model, deployment_name, api_version, endpoint, prompt):
    try:
        if provider == "OpenAI":
            if not OPENAI_AVAILABLE:
                return "Erro: Biblioteca OpenAI n√£o instalada."
            client = OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        
        elif provider == "Anthropic":
            if not ANTHROPIC_AVAILABLE:
                return "Erro: Biblioteca Anthropic n√£o instalada."
            client = Anthropic(api_key=api_key)
            response = client.messages.create(
                model=model,
                max_tokens=1024,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text

        elif provider == "Azure OpenAI":
            if not OPENAI_AVAILABLE:
                return "Erro: Biblioteca OpenAI (para Azure) n√£o instalada."
            if not endpoint or not deployment_name:
                 return "Erro: Para Azure OpenAI, o Endpoint e o Nome da Implanta√ß√£o s√£o obrigat√≥rios."
            client = AzureOpenAI(
                api_key=api_key,
                api_version=api_version if api_version else "2024-02-15-preview",
                azure_endpoint=endpoint
            )
            response = client.chat.completions.create(
                model=deployment_name, # No Azure, model √© o nome da implanta√ß√£o
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content

        elif provider == "Meta (via LiteLLM)":
            if not LITELLM_AVAILABLE:
                return "Erro: Biblioteca LiteLLM n√£o instalada."
            os.environ["OPENAI_API_KEY"] = api_key # LiteLLM usa essa var se n√£o achar outra espec√≠fica
            if "meta-llama" in model:
                 pass 

            response = completion(
                model=model, 
                messages=[{"role": "user", "content": prompt}],
                api_key=api_key 
            )
            return response.choices[0].message.content

        else:
            return f"Erro: Provedor {provider} n√£o suportado."

    except Exception as e:
        return f"Erro ao gerar hist√≥ria com {provider}: {str(e)}"

def construir_prompt_adaptado(faixa_etaria, tema):
    # Documenta√ß√£o de refer√™ncia para adapta√ß√£o: /home/ubuntu/praticas_historias_infantis.md
    prompt_base = f"Voc√™ √© um contador de hist√≥rias infantis especialista em adaptar narrativas para diferentes faixas et√°rias. " \
                  f"Crie uma hist√≥ria infantil sobre '{tema}'. "

    if faixa_etaria == "0-12 meses":
        prompt_base += "A hist√≥ria deve ser extremamente simples, com frases muito curtas, quase como uma can√ß√£o de ninar. " \
                       "Use muitas repeti√ß√µes de sons e palavras simples. O foco √© no ritmo e na sonoridade. " \
                       "Inclua elementos que remetam a texturas e sensa√ß√µes t√°teis de forma imaginativa. " \
                       "Exemplo de tom: 'O gatinho miau, miau, fofinho. A bolinha piu, piu, colorida.'"
    elif faixa_etaria == "1-3 anos":
        prompt_base += "A hist√≥ria deve ter um enredo linear e f√°cil de seguir, com poucos personagens (animais s√£o √≥timos). " \
                       "Use frases curtas e vocabul√°rio concreto e familiar. Repeti√ß√µes de frases ou a√ß√µes s√£o muito bem-vindas. " \
                       "A hist√≥ria pode envolver emo√ß√µes b√°sicas e a√ß√µes claras. " \
                       "Exemplo de tom: 'O cachorrinho correu, correu. Ele achou um osso! Que feliz!'"
    elif faixa_etaria == "4-6 anos":
        prompt_base += "A hist√≥ria pode ter um enredo um pouco mais elaborado, com um pequeno desafio ou aventura. " \
                       "Pode incluir di√°logos simples entre os personagens. O vocabul√°rio pode ser um pouco mais rico, mas ainda acess√≠vel. " \
                       "Uma moral simples ou uma li√ß√£o sobre amizade, coragem, ou curiosidade pode ser inclu√≠da sutilmente. " \
                       "Exemplo de tom: 'A pequena estrela decidiu explorar o c√©u. Ela encontrou um cometa amigo que lhe ensinou a brilhar mais forte.'"
    return prompt_base

# --- Interface do Usu√°rio ---
st.title("üß∏ Gerador de Hist√≥rias Infantis M√°gicas ‚ú®")
st.markdown("---_Transforme ideias em contos encantadores para os pequenos!_---")

# Instru√ß√µes de Uso
with st.expander("üìò Como Funciona?", expanded=False):
    st.markdown("""
    Bem-vindo ao Gerador de Hist√≥rias Infantis!

    1.  **Escolha a Faixa Et√°ria:** Selecione a idade da crian√ßa para que a hist√≥ria seja adaptada.
    2.  **Insira sua Chave de API:** Cole sua chave de API do servi√ßo LLM que voc√™ deseja usar.
    3.  **Selecione o Provedor e Modelo LLM:** Escolha o provedor (ex: OpenAI) e, em seguida, o modelo espec√≠fico (ex: gpt-4).
        *   **Para Azure OpenAI:** Voc√™ precisar√° fornecer tamb√©m o **Endpoint**, a **Vers√£o da API** (ex: 2024-02-15-preview) e o **Nome da Implanta√ß√£o** do seu modelo.
        *   **Para Meta (via LiteLLM):** Certifique-se de que seu ambiente LiteLLM esteja configurado para acessar os modelos Meta (pode exigir chaves adicionais como HUGGINGFACE_API_KEY configuradas como vari√°veis de ambiente).
    4.  **Defina a Tem√°tica:** Escreva sobre o que ser√° a hist√≥ria (ex: amizade, coragem, um animal espec√≠fico).
    5.  **Clique em Gerar:** Deixe a magia acontecer!
    """)

st.sidebar.header("üé® Configura√ß√µes da Hist√≥ria")
faixa_etaria_opcoes = ["0-12 meses", "1-3 anos", "4-6 anos"]
faixa_etaria_selecionada = st.sidebar.selectbox("üéØ Para qual faixa et√°ria √© a hist√≥ria?", faixa_etaria_opcoes)

api_key_input = st.sidebar.text_input("üîë Sua Chave de API", type="password", placeholder="Cole sua chave aqui")

# Filtrar provedores dispon√≠veis (bibliotecas instaladas)
provedores_disponiveis = {p: d for p, d in llm_providers.items() if d["available"]}
if not provedores_disponiveis:
    st.sidebar.error("Nenhuma biblioteca de LLM foi encontrada. Por favor, instale as depend√™ncias necess√°rias (OpenAI, Anthropic, LiteLLM).")
    st.stop()

llm_provider_selecionado = st.sidebar.selectbox("ü§ñ Provedor LLM", options=list(provedores_disponiveis.keys()))

azure_endpoint_input = None
azure_api_version_input = None
azure_deployment_name_input = None
llm_model_selecionado = None

if llm_provider_selecionado:
    provider_details = provedores_disponiveis[llm_provider_selecionado]
    modelos_do_provedor = provider_details["models"]
    
    if provider_details.get("requires_endpoint"):
        st.sidebar.markdown("**Configura√ß√µes Adicionais para Azure OpenAI:**")
        azure_endpoint_input = st.sidebar.text_input("üåê Endpoint do Azure", placeholder="https://seu-recurso.openai.azure.com/")
        azure_api_version_input = st.sidebar.text_input("üóìÔ∏è Vers√£o da API Azure", placeholder="2024-02-15-preview")
        azure_deployment_name_input = st.sidebar.text_input("‚öôÔ∏è Nome da Implanta√ß√£o Azure", placeholder="nome-da-sua-implantacao-gpt4")
        llm_model_selecionado = azure_deployment_name_input 
    else:
        llm_model_selecionado = st.sidebar.selectbox("‚öôÔ∏è Modelo Espec√≠fico", options=modelos_do_provedor)

tematica_input = st.sidebar.text_area("üí° Qual ser√° o tema da hist√≥ria?", placeholder="Ex: A aventura de um coelhinho curioso na floresta")

if st.sidebar.button("ü™Ñ Gerar Hist√≥ria!"):
    if not api_key_input:
        st.sidebar.error("Por favor, insira sua Chave de API.")
    elif not tematica_input:
        st.sidebar.error("Por favor, digite uma tem√°tica para a hist√≥ria.")
    elif not llm_provider_selecionado:
        st.sidebar.error("Por favor, selecione um provedor LLM.")
    elif provider_details.get("requires_endpoint") and (not azure_endpoint_input or not azure_deployment_name_input):
        st.sidebar.error("Para Azure OpenAI, o Endpoint e o Nome da Implanta√ß√£o s√£o obrigat√≥rios.")
    elif not llm_model_selecionado and not provider_details.get("requires_endpoint"):
        st.sidebar.error("Por favor, selecione um modelo LLM espec√≠fico.")
    else:
        with st.spinner(f"Gerando hist√≥ria com {llm_provider_selecionado}..."):
            prompt_final = construir_prompt_adaptado(faixa_etaria_selecionada, tematica_input)
            
            nome_modelo_ou_implantacao = llm_model_selecionado
            if llm_provider_selecionado == "Azure OpenAI":
                modelo_param = None 
                implantacao_param = azure_deployment_name_input
            else:
                modelo_param = llm_model_selecionado
                implantacao_param = None

            historia_gerada = gerar_historia_com_llm(
                api_key=api_key_input,
                provider=llm_provider_selecionado,
                model=modelo_param, 
                deployment_name=implantacao_param, 
                api_version=azure_api_version_input,
                endpoint=azure_endpoint_input,
                prompt=prompt_final
            )
            st.subheader("üìñ Sua Hist√≥ria Personalizada:")
            st.markdown(f"<div style='background-color: #FFFACD; padding: 15px; border-radius: 10px; border: 2px solid #FFD700;'>{historia_gerada}</div>", unsafe_allow_html=True)

st.markdown("<br><br><hr>", unsafe_allow_html=True)
st.markdown("""
<style>
.footer {
    position: fixed;
    left: 0;
    bottom: 0;
    width: 100%;
    background-color: #f1f1f1; /* Voc√™ pode mudar a cor de fundo */
    color: grey;
    text-align: center;
    padding: 10px;
    font-size: 14px;
}
.footer a {
    color: #007bff; /* Cor dos links */
    text-decoration: none;
    margin: 0 10px;
}
.footer a:hover {
    text-decoration: underline;
}
.footer .icon {
    margin-right: 5px;
}
</style>

<div class="footer">
    <p>
        Constru√≠do por Pedro William Ribeiro Diniz com Manus
        <br>
        <a href="https://www.linkedin.com/in/pedrowilliamrd/" target="_blank">
            <span class="icon">üîó</span>LinkedIn
        </a> |
        <a href="https://github.com/pedrow28" target="_blank">
            <span class="icon">üíª</span>GitHub
        </a> |
        <a href="https://manus.im/app" target="_blank">
            <span class="icon">üí°</span>Manus
        </a>
    </p>
</div>
""", unsafe_allow_html=True)

# Para executar este aplicativo: salve como app.py e rode `streamlit run app.py` no terminal.
